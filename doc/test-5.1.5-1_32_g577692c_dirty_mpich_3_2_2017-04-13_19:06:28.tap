TAP version 13
# MPICH test suite results (TAP format)
# date 2017-04-13-19-06
ok 1 - ./parastation/mprobe 2
ok 2 - ./parastation/locknull 2
ok 3 - ./parastation/get-acc-loc 4
ok 4 - ./parastation/win_sync_err_flush 2
ok 5 - ./parastation/win_sync_err_flushall 2
ok 6 - ./parastation/win_sync_err_lockall_pt 2
ok 7 - ./parastation/win_sync_err_lockall_at 2
ok 8 - ./parastation/win_sync_err_unlockall 2
ok 9 - ./parastation/win_sync_err_reqops 2
ok 10 - ./parastation/win_flavor_err_shm 2
ok 11 - ./parastation/win_flavor_err_dyn 2
ok 12 - ./parastation/spawn_isend 2
ok 13 - ./parastation/spawn_ssend 2
ok 14 - ./parastation/spawn_anysrc 2
ok 15 - ./parastation/spawn_rma 2
ok 16 - ./parastation/spawn_universe 2
ok 17 - ./parastation/num_contexts_2k 1
ok 18 - ./parastation/num_contexts_4k 1
ok 19 - ./parastation/num_contexts_8k 1
ok 20 - ./parastation/num_contexts_16k 1
ok 21 - ./attr/attrt 2
ok 22 - ./attr/attric 4
ok 23 - ./attr/attrerr 1
ok 24 - ./attr/attrend 1
ok 25 - ./attr/attrend 4
ok 26 - ./attr/attrend2 1
ok 27 - ./attr/attrend2 5
ok 28 - ./attr/attrerrcomm 1
ok 29 - ./attr/attrerrtype 1
ok 30 - ./attr/attrdeleteget 1
ok 31 - ./attr/attr2type 1
ok 32 - ./attr/attrorder 1
ok 33 - ./attr/attrordercomm 1
ok 34 - ./attr/attrordertype 1
ok 35 - ./attr/baseattr2 1
ok 36 - ./attr/baseattrcomm 1
ok 37 - ./attr/fkeyval 1
ok 38 - ./attr/fkeyvalcomm 1
ok 39 - ./attr/fkeyvaltype 1
ok 40 - ./attr/keyval_double_free 1
ok 41 - ./attr/keyval_double_free_comm 1
ok 42 - ./attr/keyval_double_free_type 1
ok 43 - ./attr/keyval_double_free_win 1
ok 44 - ./coll/allred 4
ok 45 - ./coll/allred 7
ok 46 - ./coll/allred 4
ok 47 - ./coll/allredmany 4
ok 48 - ./coll/allred2 4
ok 49 - ./coll/allred3 10
ok 50 - ./coll/allred4 4
ok 51 - ./coll/allred5 5
ok 52 - ./coll/allred5 10
ok 53 - ./coll/allred6 4
ok 54 - ./coll/allred6 7
ok 55 - ./coll/reduce 5
ok 56 - ./coll/reduce 10
ok 57 - ./coll/reduce_local 2
ok 58 - ./coll/op_commutative 2
ok 59 - ./coll/red3 10
ok 60 - ./coll/red4 10
ok 61 - ./coll/alltoall1 8
ok 62 - ./coll/alltoallv 10
ok 63 - ./coll/alltoallv0 10
ok 64 - ./coll/alltoallw1 10
ok 65 - ./coll/alltoallw2 10
ok 66 - ./coll/alltoallw_zeros 1
ok 67 - ./coll/alltoallw_zeros 2
ok 68 - ./coll/alltoallw_zeros 5
ok 69 - ./coll/alltoallw_zeros 8
ok 70 - ./coll/allgather2 10
ok 71 - ./coll/allgather3 10
ok 72 - ./coll/allgatherv2 10
ok 73 - ./coll/allgatherv3 10
ok 74 - ./coll/allgatherv4 4
ok 75 - ./coll/allgather_struct 10
ok 76 - ./coll/bcasttest 4
ok 77 - ./coll/bcasttest 8
ok 78 - ./coll/bcasttest 10
ok 79 - ./coll/bcast_full 4
ok 80 - ./coll/bcast_min_datatypes 10
ok 81 - ./coll/bcast_comm_world 10
ok 82 - ./coll/bcastzerotype 1
ok 83 - ./coll/bcastzerotype 4
ok 84 - ./coll/bcastzerotype 5
ok 85 - ./coll/bcastzerotype 10
ok 86 - ./coll/coll2 5
ok 87 - ./coll/coll3 5
ok 88 - ./coll/coll4 4
ok 89 - ./coll/coll5 4
ok 90 - ./coll/coll6 5
ok 91 - ./coll/coll7 1
ok 92 - ./coll/coll7 2
ok 93 - ./coll/coll7 5
ok 94 - ./coll/coll8 4
ok 95 - ./coll/coll9 4
ok 96 - ./coll/coll10 4
ok 97 - ./coll/coll11 4
ok 98 - ./coll/coll12 4
ok 99 - ./coll/coll13 4
ok 100 - ./coll/longuser 4
ok 101 - ./coll/redscat 4
ok 102 - ./coll/redscat 6
ok 103 - ./coll/redscat2 4
ok 104 - ./coll/redscat2 5
ok 105 - ./coll/redscat2 10
ok 106 - ./coll/redscat3 8
ok 107 - ./coll/redscatinter 8
ok 108 - ./coll/red_scat_block 4
ok 109 - ./coll/red_scat_block 5
ok 110 - ./coll/red_scat_block 8
ok 111 - ./coll/red_scat_block2 4
ok 112 - ./coll/red_scat_block2 5
ok 113 - ./coll/red_scat_block2 10
ok 114 - ./coll/redscatblk3 8
ok 115 - ./coll/redscatblk3 10
ok 116 - ./coll/redscatbkinter 8
ok 117 - ./coll/redscatbkinter 10
ok 118 - ./coll/scantst 4
ok 119 - ./coll/exscan 10
ok 120 - ./coll/exscan2 5
ok 121 - ./coll/gather 4
ok 122 - ./coll/gather2 4
ok 123 - ./coll/scattern 4
ok 124 - ./coll/scatter2 4
ok 125 - ./coll/scatter3 4
ok 126 - ./coll/scatterv 4
ok 127 - ./coll/icbcast 4
ok 128 - ./coll/icbcast 10
ok 129 - ./coll/icallreduce 5
ok 130 - ./coll/icallreduce 7
ok 131 - ./coll/icreduce 5
ok 132 - ./coll/icreduce 7
ok 133 - ./coll/icscatter 5
ok 134 - ./coll/icscatter 7
ok 135 - ./coll/icgather 5
ok 136 - ./coll/icgather 7
ok 137 - ./coll/icallgather 5
ok 138 - ./coll/icallgather 7
ok 139 - ./coll/icbarrier 5
ok 140 - ./coll/icbarrier 7
ok 141 - ./coll/icallgatherv 5
ok 142 - ./coll/icallgatherv 7
ok 143 - ./coll/icgatherv 5
ok 144 - ./coll/icgatherv 7
ok 145 - ./coll/icscatterv 5
ok 146 - ./coll/icscatterv 7
ok 147 - ./coll/icalltoall 5
ok 148 - ./coll/icalltoall 7
ok 149 - ./coll/icalltoallv 5
ok 150 - ./coll/icalltoallv 7
ok 151 - ./coll/icalltoallw 5
ok 152 - ./coll/icalltoallw 7
ok 153 - ./coll/opland 4
ok 154 - ./coll/oplor 4
ok 155 - ./coll/oplxor 4
ok 156 - ./coll/oplxor 5
ok 157 - ./coll/opband 4
ok 158 - ./coll/opbor 4
ok 159 - ./coll/opbxor 4
ok 160 - ./coll/opbxor 5
ok 161 - ./coll/opprod 5
ok 162 - ./coll/opprod 6
ok 163 - ./coll/opsum 4
ok 164 - ./coll/opmin 4
ok 165 - ./coll/opminloc 4
ok 166 - ./coll/opmax 5
ok 167 - ./coll/opmaxloc 5
ok 168 - ./coll/uoplong 4
ok 169 - ./coll/uoplong 11
ok 170 - ./coll/uoplong 16
not ok 171 - ./coll/nonblocking 4
  ---
  Directory: ./coll
  File: nonblocking
  Num-procs: 4
  Date: "Thu Apr 13 19:23:38 2017"
  ...
## Test output (expected 'No Errors'):
not ok 172 - ./coll/nonblocking 5
  ---
  Directory: ./coll
  File: nonblocking
  Num-procs: 5
  Date: "Thu Apr 13 19:26:38 2017"
  ...
## Test output (expected 'No Errors'):
not ok 173 - ./coll/nonblocking 10
  ---
  Directory: ./coll
  File: nonblocking
  Num-procs: 10
  Date: "Thu Apr 13 19:29:38 2017"
  ...
## Test output (expected 'No Errors'):
not ok 174 - ./coll/nonblocking2 1
  ---
  Directory: ./coll
  File: nonblocking2
  Num-procs: 1
  Date: "Thu Apr 13 19:29:38 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 175 - ./coll/nonblocking2 4
  ---
  Directory: ./coll
  File: nonblocking2
  Num-procs: 4
  Date: "Thu Apr 13 19:29:38 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 176 - ./coll/nonblocking2 5
  ---
  Directory: ./coll
  File: nonblocking2
  Num-procs: 5
  Date: "Thu Apr 13 19:29:39 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 177 - ./coll/nonblocking2 10
  ---
  Directory: ./coll
  File: nonblocking2
  Num-procs: 10
  Date: "Thu Apr 13 19:29:39 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nonblocking2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 178 - ./coll/nonblocking3 1
  ---
  Directory: ./coll
  File: nonblocking3
  Num-procs: 1
  Date: "Thu Apr 13 19:29:39 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 179 - ./coll/nonblocking3 4
  ---
  Directory: ./coll
  File: nonblocking3
  Num-procs: 4
  Date: "Thu Apr 13 19:29:39 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 180 - ./coll/nonblocking3 5
  ---
  Directory: ./coll
  File: nonblocking3
  Num-procs: 5
  Date: "Thu Apr 13 19:29:40 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 181 - ./coll/nonblocking3 10
  ---
  Directory: ./coll
  File: nonblocking3
  Num-procs: 10
  Date: "Thu Apr 13 19:29:40 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nonblocking3: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 182 - ./coll/iallred 2
  ---
  Directory: ./coll
  File: iallred
  Num-procs: 2
  Date: "Thu Apr 13 19:32:40 2017"
  ...
## Test output (expected 'No Errors'):
not ok 183 - ./coll/ibarrier 2
  ---
  Directory: ./coll
  File: ibarrier
  Num-procs: 2
  Date: "Thu Apr 13 19:33:10 2017"
  ...
## Test output (expected 'No Errors'):
not ok 184 - ./coll/nballtoall1 8
  ---
  Directory: ./coll
  File: nballtoall1
  Num-procs: 8
  Date: "Thu Apr 13 19:36:10 2017"
  ...
## Test output (expected 'No Errors'):
not ok 185 - ./coll/nbcoll2 5
  ---
  Directory: ./coll
  File: nbcoll2
  Num-procs: 5
  Date: "Thu Apr 13 19:36:11 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbcoll2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbcoll2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbcoll2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbcoll2: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 186 - ./coll/nbredscat 4
  ---
  Directory: ./coll
  File: nbredscat
  Num-procs: 4
  Date: "Thu Apr 13 19:39:11 2017"
  ...
## Test output (expected 'No Errors'):
not ok 187 - ./coll/nbredscat 8
  ---
  Directory: ./coll
  File: nbredscat
  Num-procs: 8
  Date: "Thu Apr 13 19:42:11 2017"
  ...
## Test output (expected 'No Errors'):
not ok 188 - ./coll/nbredscat3 8
  ---
  Directory: ./coll
  File: nbredscat3
  Num-procs: 8
  Date: "Thu Apr 13 19:45:11 2017"
  ...
## Test output (expected 'No Errors'):
not ok 189 - ./coll/nbredscatinter 8
  ---
  Directory: ./coll
  File: nbredscatinter
  Num-procs: 8
  Date: "Thu Apr 13 19:48:11 2017"
  ...
## Test output (expected 'No Errors'):
not ok 190 - ./coll/nbicbcast 8
  ---
  Directory: ./coll
  File: nbicbcast
  Num-procs: 8
  Date: "Thu Apr 13 19:48:11 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicbcast: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nbicbcast: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicbcast: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicbcast: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicbcast: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 191 - ./coll/nbicallreduce 8
  ---
  Directory: ./coll
  File: nbicallreduce
  Num-procs: 8
  Date: "Thu Apr 13 19:51:11 2017"
  ...
## Test output (expected 'No Errors'):
not ok 192 - ./coll/nbicreduce 8
  ---
  Directory: ./coll
  File: nbicreduce
  Num-procs: 8
  Date: "Thu Apr 13 19:51:12 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicreduce: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicreduce: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicreduce: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicreduce: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicreduce: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicreduce: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 193 - ./coll/nbicscatter 8
  ---
  Directory: ./coll
  File: nbicscatter
  Num-procs: 8
  Date: "Thu Apr 13 19:51:12 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatter: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nbicscatter: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatter: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatter: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatter: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatter: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatter: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatter: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 194 - ./coll/nbicgather 8
  ---
  Directory: ./coll
  File: nbicgather
  Num-procs: 8
  Date: "Thu Apr 13 19:51:12 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgather: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgather: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgather: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgather: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgather: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgather: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgather: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 195 - ./coll/nbicallgather 8
  ---
  Directory: ./coll
  File: nbicallgather
  Num-procs: 8
  Date: "Thu Apr 13 19:54:12 2017"
  ...
## Test output (expected 'No Errors'):
not ok 196 - ./coll/nbicbarrier 8
  ---
  Directory: ./coll
  File: nbicbarrier
  Num-procs: 8
  Date: "Thu Apr 13 19:57:12 2017"
  ...
## Test output (expected 'No Errors'):
not ok 197 - ./coll/nbicallgatherv 8
  ---
  Directory: ./coll
  File: nbicallgatherv
  Num-procs: 8
  Date: "Thu Apr 13 20:00:12 2017"
  ...
## Test output (expected 'No Errors'):
not ok 198 - ./coll/nbicgatherv 8
  ---
  Directory: ./coll
  File: nbicgatherv
  Num-procs: 8
  Date: "Thu Apr 13 20:00:13 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgatherv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgatherv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgatherv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgatherv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgatherv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgatherv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgatherv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicgatherv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 199 - ./coll/nbicscatterv 8
  ---
  Directory: ./coll
  File: nbicscatterv
  Num-procs: 8
  Date: "Thu Apr 13 20:00:13 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatterv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatterv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nbicscatterv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatterv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatterv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatterv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicscatterv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## nbicscatterv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 200 - ./coll/nbicalltoall 8
  ---
  Directory: ./coll
  File: nbicalltoall
  Num-procs: 8
  Date: "Thu Apr 13 20:03:13 2017"
  ...
## Test output (expected 'No Errors'):
not ok 201 - ./coll/nbicalltoallv 8
  ---
  Directory: ./coll
  File: nbicalltoallv
  Num-procs: 8
  Date: "Thu Apr 13 20:03:13 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicalltoallv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## nbicalltoallv: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:125: MPID_Progress_wait: Assertion `mpi_errno == 0' failed.
not ok 202 - ./coll/nbicalltoallw 8
  ---
  Directory: ./coll
  File: nbicalltoallw
  Num-procs: 8
  Date: "Thu Apr 13 20:06:13 2017"
  ...
## Test output (expected 'No Errors'):
ok 203 - ./comm/dup 2
ok 204 - ./comm/dupic 4
ok 205 - ./comm/commcreate1 8
ok 206 - ./comm/commname 4
ok 207 - ./comm/ic1 4
ok 208 - ./comm/ic2 33
ok 209 - ./comm/icgroup 8
ok 210 - ./comm/icm 8
ok 211 - ./comm/icsplit 8
ok 212 - ./comm/iccreate 8
ok 213 - ./comm/ctxalloc 2
ok 214 - ./comm/ctxsplit 4
ok 215 - ./comm/cmfree 4
ok 216 - ./comm/cmsplit 4
ok 217 - ./comm/cmsplit2 12
ok 218 - ./comm/probe-intercomm 2
ok 219 - ./comm/cmsplit_type 4
ok 220 - ./comm/comm_create_group 4
ok 221 - ./comm/comm_create_group 8
ok 222 - ./comm/comm_group_half 2
ok 223 - ./comm/comm_group_half 4
ok 224 - ./comm/comm_group_half 8
ok 225 - ./comm/comm_group_rand 2
ok 226 - ./comm/comm_group_rand 4
ok 227 - ./comm/comm_group_rand 8
not ok 228 - ./comm/comm_idup 2
  ---
  Directory: ./comm
  File: comm_idup
  Num-procs: 2
  Date: "Thu Apr 13 20:09:22 2017"
  ...
## Test output (expected 'No Errors'):
not ok 229 - ./comm/comm_idup 4
  ---
  Directory: ./comm
  File: comm_idup
  Num-procs: 4
  Date: "Thu Apr 13 20:12:22 2017"
  ...
## Test output (expected 'No Errors'):
not ok 230 - ./comm/comm_idup 9
  ---
  Directory: ./comm
  File: comm_idup
  Num-procs: 9
  Date: "Thu Apr 13 20:15:23 2017"
  ...
## Test output (expected 'No Errors'):
not ok 231 - ./comm/comm_idup_mul 2
  ---
  Directory: ./comm
  File: comm_idup_mul
  Num-procs: 2
  Date: "Thu Apr 13 20:18:23 2017"
  ...
## Test output (expected 'No Errors'):
not ok 232 - ./comm/comm_idup_overlap 2
  ---
  Directory: ./comm
  File: comm_idup_overlap
  Num-procs: 2
  Date: "Thu Apr 13 20:21:23 2017"
  ...
## Test output (expected 'No Errors'):
not ok 233 - ./comm/comm_idup_iallreduce 6
  ---
  Directory: ./comm
  File: comm_idup_iallreduce
  Num-procs: 6
  Date: "Thu Apr 13 20:24:23 2017"
  ...
## Test output (expected 'No Errors'):
not ok 234 - ./comm/comm_idup_nb 6
  ---
  Directory: ./comm
  File: comm_idup_nb
  Num-procs: 6
  Date: "Thu Apr 13 20:27:23 2017"
  ...
## Test output (expected 'No Errors'):
not ok 235 - ./comm/comm_idup_isend 6
  ---
  Directory: ./comm
  File: comm_idup_isend
  Num-procs: 6
  Date: "Thu Apr 13 20:30:23 2017"
  ...
## Test output (expected 'No Errors'):
not ok 236 - ./comm/comm_idup_comm 6
  ---
  Directory: ./comm
  File: comm_idup_comm
  Num-procs: 6
  Date: "Thu Apr 13 20:33:24 2017"
  ...
## Test output (expected 'No Errors'):
not ok 237 - ./comm/comm_idup_comm2 6
  ---
  Directory: ./comm
  File: comm_idup_comm2
  Num-procs: 6
  Date: "Thu Apr 13 20:36:24 2017"
  ...
## Test output (expected 'No Errors'):
ok 238 - ./comm/dup_with_info 2
ok 239 - ./comm/dup_with_info 4
ok 240 - ./comm/dup_with_info 9
ok 241 - ./comm/comm_info 6
not ok 242 - ./comm/comm_create_group_idup 4
  ---
  Directory: ./comm
  File: comm_create_group_idup
  Num-procs: 4
  Date: "Thu Apr 13 20:39:25 2017"
  ...
## Test output (expected 'No Errors'):
ok 243 - ./datatype/contents 1
ok 244 - ./datatype/gaddress 1
ok 245 - ./datatype/lbub 1
ok 246 - ./datatype/localpack 1
ok 247 - ./datatype/simple-pack 1
ok 248 - ./datatype/simple-pack-external 1
ok 249 - ./datatype/transpose-pack 1
ok 250 - ./datatype/slice-pack 1
ok 251 - ./datatype/struct-pack 1
ok 252 - ./datatype/structpack2 1
ok 253 - ./datatype/typecommit 1
ok 254 - ./datatype/typename 1
ok 255 - ./datatype/typefree 1
ok 256 - ./datatype/zeroparms 1
ok 257 - ./datatype/getpartelm 2
ok 258 - ./datatype/tresized 2
ok 259 - ./datatype/tresized2 2
ok 260 - ./datatype/sendrecvt2 2
ok 261 - ./datatype/sendrecvt4 2
ok 262 - ./datatype/tmatchsize 1
ok 263 - ./datatype/tfree 2
ok 264 - ./datatype/typelb 1
ok 265 - ./datatype/contigstruct 1
ok 266 - ./datatype/struct-zero-count 1
ok 267 - ./datatype/blockindexed-zero-count 1
ok 268 - ./datatype/blockindexed-misc 1
ok 269 - ./datatype/indexed-misc 1
ok 270 - ./datatype/subarray-pack 1
ok 271 - ./datatype/subarray 2
ok 272 - ./datatype/darray-pack 1
ok 273 - ./datatype/darray-pack 9
not ok 274 - ./datatype/darray-pack 72
  ---
  Directory: ./datatype
  File: darray-pack
  Num-procs: 72
  Date: "Thu Apr 13 20:42:31 2017"
  ...
## Test output (expected 'No Errors'):
ok 275 - ./datatype/darray-cyclic 12
ok 276 - ./datatype/pairtype-size-extent 1
ok 277 - ./datatype/simple-commit 1
ok 278 - ./datatype/simple-size-extent 1
ok 279 - ./datatype/struct-no-real-types 1
ok 280 - ./datatype/struct-empty-el 1
ok 281 - ./datatype/contig-zero-count 1
ok 282 - ./datatype/simple-resized 1
ok 283 - ./datatype/unusual-noncontigs 1
ok 284 - ./datatype/hindexed-zeros 1
ok 285 - ./datatype/lots-of-types 1
ok 286 - ./datatype/get-elements-pairtype 1
ok 287 - ./datatype/unpack 1
ok 288 - ./datatype/struct-ezhov 1
ok 289 - ./datatype/zeroblks 1
ok 290 - ./datatype/struct-derived-zeros 1
ok 291 - ./datatype/struct-verydeep 1
ok 292 - ./datatype/get-elements 1
ok 293 - ./datatype/hindexed_block 1
ok 294 - ./datatype/hindexed_block_contents 1
ok 295 - ./datatype/vecblklen 1
ok 296 - ./datatype/hvecblklen 1
ok 297 - ./datatype/longdouble 1
ok 298 - ./datatype/dataalign 2
ok 299 - ./datatype/cxx-types 1
ok 300 - ./errhan/adderr 1
ok 301 - ./errhan/commcall 2
ok 302 - ./errhan/errfatal 1
ok 303 - ./errhan/predef_eh 1
ok 304 - ./errhan/predef_eh 2
ok 305 - ./errhan/errstring2 1
ok 306 - ./errhan/dynamic_errcode_predefined_errclass 1
ok 307 - ./group/groupcreate 4
ok 308 - ./group/grouptest 8
ok 309 - ./group/grouptest2 4
ok 310 - ./group/groupnullincl 4
ok 311 - ./group/gtranks 8
ok 312 - ./group/gtranksperf 20
ok 313 - ./info/infodup 1
ok 314 - ./info/infodel 1
ok 315 - ./info/infovallen 1
ok 316 - ./info/infoorder 1
ok 317 - ./info/infomany 1
ok 318 - ./info/infomany2 1
ok 319 - ./info/infotest 1
ok 320 - ./info/infoget 1
ok 321 - ./info/infoenv 1
ok 322 - ./init/exitst1 2
ok 323 - ./init/exitst2 4
ok 324 - ./init/initstat 1
ok 325 - ./init/timeout 2
ok 326 - ./init/version 1
ok 327 - ./init/finalized 1
ok 328 - ./init/attrself 1
ok 329 - ./init/library_version 1
ok 330 - ./mpi_t/mpi_t_str 1
ok 331 - ./mpi_t/mpit_vars 1
ok 332 - ./mpi_t/cvarwrite 1
ok 333 - ./mpi_t/getindex 1
ok 334 - ./pt2pt/sendrecv1 4
ok 335 - ./pt2pt/sendrecv2 2
ok 336 - ./pt2pt/sendrecv3 2
ok 337 - ./pt2pt/sendflood 8
ok 338 - ./pt2pt/sendself 1
ok 339 - ./pt2pt/sendall 4
ok 340 - ./pt2pt/anyall 2
ok 341 - ./pt2pt/eagerdt 2
ok 342 - ./pt2pt/pingping 2
ok 343 - ./pt2pt/bottom 2
ok 344 - ./pt2pt/bsend1 1
ok 345 - ./pt2pt/bsend2 1
not ok 346 - ./pt2pt/bsend3 1
  ---
  Directory: ./pt2pt
  File: bsend3
  Num-procs: 1
  Date: "Thu Apr 13 20:46:02 2017"
  ...
## Test output (expected 'No Errors'):
## Deadlock detected! Process 2939 will wait forever.
## ('wait' called without outstanding send or recv requests).
## Requests:
## Sockets:
##   sock#0x1d82d40 listen:    -1 demand:   0(   0)  src:(134.94.169.164,2939,(nil),r0000000) anyrecv:     0
##   Connections:
##     con#0x1d85708 type:  loop state:    open dest:(134.94.169.164,2939,0x1d85708,r0000000) recvcnt:    0
## Fds:
## 
## Reqs:6 GenReqs: (cnt:1  used:0)
not ok 347 - ./pt2pt/bsend4 1
  ---
  Directory: ./pt2pt
  File: bsend4
  Num-procs: 1
  Date: "Thu Apr 13 20:46:03 2017"
  ...
## Test output (expected 'No Errors'):
## Deadlock detected! Process 2988 will wait forever.
## ('wait' called without outstanding send or recv requests).
## Requests:
## Sockets:
##   sock#0x9fed40 listen:    -1 demand:   0(   0)  src:(134.94.169.164,2988,(nil),r0000000) anyrecv:     0
##   Connections:
##     con#0xa01708 type:  loop state:    open dest:(134.94.169.164,2988,0xa01708,r0000000) recvcnt:    0
## Fds:
## 
## Reqs:5 GenReqs: (cnt:1  used:0)
ok 348 - ./pt2pt/bsend5 4
ok 349 - ./pt2pt/bsendalign 2
ok 350 - ./pt2pt/bsendpending 2
ok 351 - ./pt2pt/isendself 1
ok 352 - ./pt2pt/issendselfcancel 1
ok 353 - ./pt2pt/isendirecv 10
ok 354 - ./pt2pt/bsendfrag 2
ok 355 - ./pt2pt/icsend 4
ok 356 - ./pt2pt/rqstatus 2
not ok 357 - ./pt2pt/rqfreeb 4
  ---
  Directory: ./pt2pt
  File: rqfreeb
  Num-procs: 4
  Date: "Thu Apr 13 20:46:13 2017"
  ...
## Test output (expected 'No Errors'):
##  No Errors
## In direct memory block for handle type REQUEST, 1 handles are still allocated
## [0] 56 at [0x0000000000a7ab18], ss/Work/psmpi/mpich2/src/mpi/pt2pt/greq_start.c[96]
not ok 358 - ./pt2pt/greq1 1
  ---
  Directory: ./pt2pt
  File: greq1
  Num-procs: 1
  Date: "Thu Apr 13 20:46:14 2017"
  ...
## Test output (expected 'No Errors'):
## Deadlock detected! Process 3615 will wait forever.
## ('wait' called without outstanding send or recv requests).
## Requests:
## Sockets:
##   sock#0x23ddd40 listen:    -1 demand:   0(   0)  src:(134.94.169.164,3615,(nil),r0000000) anyrecv:     0
##   Connections:
##     con#0x23e0708 type:  loop state:    open dest:(134.94.169.164,3615,0x23e0708,r0000000) recvcnt:    0
## Fds:
## 
## Reqs:3 GenReqs: (cnt:0  used:0)
ok 359 - ./pt2pt/probe-unexp 4
ok 360 - ./pt2pt/probenull 1
ok 361 - ./pt2pt/scancel 2  # SKIP xfail tests disabled
ok 361 - ./pt2pt/scancel2 2
ok 363 - ./pt2pt/pscancel 2  # SKIP xfail tests disabled
ok 362 - ./pt2pt/rcancel 2
ok 365 - ./pt2pt/cancelrecv 2  # SKIP xfail tests disabled
ok 366 - ./pt2pt/scancel_unmatch 2  # SKIP xfail tests disabled
ok 363 - ./pt2pt/cancelanysrc 2
ok 364 - ./pt2pt/isendselfprobe 1
ok 365 - ./pt2pt/inactivereq 1
ok 366 - ./pt2pt/waittestnull 1
ok 367 - ./pt2pt/waitany-null 1
ok 368 - ./pt2pt/mprobe 2
ok 369 - ./pt2pt/big_count_status 1
ok 370 - ./rma/winname 2
ok 371 - ./rma/allocmem 2
ok 372 - ./rma/putfence1 2
ok 373 - ./rma/putfidx 4
not ok 374 - ./rma/getfence1 2
  ---
  Directory: ./rma
  File: getfence1
  Num-procs: 2
  Date: "Thu Apr 13 20:49:00 2017"
  ...
## Test output (expected 'No Errors'):
## Deadlock detected! Process 4713 will wait forever.
## ('wait' called without outstanding send or recv requests).
## Requests:
## Sockets:
##   sock#0x885d40 listen:    -1 demand:   0(   0)  src:(134.94.169.164,4713,(nil),r0000000) anyrecv:     1
##   Connections:
##     con#0x8887e8 type:  loop state:    open dest:(134.94.169.164,4713,0x8887e8,r0000000) recvcnt:    1
##     con#0x888b88 type:   shm state:    norw dest:(134.94.169.164,4714,0x25e39a8,r0000001) recvcnt:    2
## Fds:
## 
## Reqs:11160 GenReqs: (cnt:724  used:724)
## ReqsAnySrc:4173 RecvQAnySrc:43085
ok 375 - ./rma/accfence1 4
ok 376 - ./rma/adlb_mimic1 3
ok 377 - ./rma/accfence2 4
ok 378 - ./rma/putpscw1 4
ok 379 - ./rma/accpscw1 4
ok 380 - ./rma/getgroup 4
ok 381 - ./rma/transpose1 2
ok 382 - ./rma/transpose2 2
ok 383 - ./rma/transpose3 2
ok 384 - ./rma/transpose3_shm 2
ok 385 - ./rma/transpose5 2
ok 386 - ./rma/transpose6 1
ok 387 - ./rma/transpose7 2
ok 388 - ./rma/test1 2
ok 389 - ./rma/test2 2
ok 390 - ./rma/test2_shm 2
ok 391 - ./rma/test3 2
ok 392 - ./rma/test3_shm 2
ok 393 - ./rma/test4 2
ok 394 - ./rma/test5 2
ok 395 - ./rma/lockcontention 3
ok 396 - ./rma/lockcontention2 4
ok 397 - ./rma/lockcontention2 8
ok 398 - ./rma/lockcontention3 8
ok 399 - ./rma/lockopts 2
ok 400 - ./rma/lock_dt 2
ok 401 - ./rma/lock_dt_flush 2
ok 402 - ./rma/lock_dt_flushlocal 2
ok 403 - ./rma/lockall_dt 4
ok 404 - ./rma/lockall_dt_flush 4
ok 405 - ./rma/lockall_dt_flushall 4
ok 406 - ./rma/lockall_dt_flushlocal 4
ok 407 - ./rma/lockall_dt_flushlocalall 4
ok 408 - ./rma/lock_contention_dt 4
ok 409 - ./rma/transpose4 2
ok 410 - ./rma/fetchandadd 7
ok 411 - ./rma/fetchandadd_tree 7
ok 412 - ./rma/wintest 2
ok 413 - ./rma/wintest_shm 2
ok 414 - ./rma/contig_displ 1
ok 415 - ./rma/test1_am 2
ok 416 - ./rma/test2_am 2
ok 417 - ./rma/test2_am_shm 2
ok 418 - ./rma/test3_am 2
ok 419 - ./rma/test3_am_shm 2
ok 420 - ./rma/test4_am 2
ok 421 - ./rma/test5_am 2
ok 422 - ./rma/fetchandadd_am 7
ok 423 - ./rma/fetchandadd_tree_am 7
ok 424 - ./rma/accfence2_am 4
ok 425 - ./rma/test1_dt 2
ok 426 - ./rma/nullpscw 7
ok 427 - ./rma/nullpscw_shm 7
ok 428 - ./rma/attrorderwin 1
ok 429 - ./rma/wincall 2
ok 430 - ./rma/baseattrwin 1
ok 431 - ./rma/fkeyvalwin 1
ok 432 - ./rma/selfrma 1
ok 433 - ./rma/mixedsync 4
ok 434 - ./rma/epochtest 4
ok 435 - ./rma/locknull 2
ok 436 - ./rma/rmanull 2
ok 437 - ./rma/rmazero 2
ok 438 - ./rma/strided_acc_indexed 2
ok 439 - ./rma/strided_acc_onelock 2
ok 440 - ./rma/strided_acc_subarray 2
ok 441 - ./rma/strided_get_indexed 2
ok 442 - ./rma/strided_putget_indexed 4
ok 443 - ./rma/strided_putget_indexed_shared 4
ok 444 - ./rma/strided_getacc_indexed 4
ok 445 - ./rma/strided_getacc_indexed_shared 4
ok 446 - ./rma/window_creation 2
ok 447 - ./rma/contention_put 4
ok 448 - ./rma/contention_putget 4
ok 449 - ./rma/put_base 2
ok 450 - ./rma/put_bottom 2
ok 451 - ./rma/win_flavors 4
ok 452 - ./rma/win_flavors 3
ok 453 - ./rma/manyrma2 2
ok 454 - ./rma/manyrma2_shm 2
ok 455 - ./rma/manyrma3 2
ok 456 - ./rma/win_shared 4
ok 457 - ./rma/win_shared_create_allocshm 4
ok 458 - ./rma/win_shared_create_no_allocshm 4
ok 459 - ./rma/win_shared_noncontig 4
ok 460 - ./rma/win_shared_noncontig_put 4
ok 461 - ./rma/win_zero 4
ok 462 - ./rma/win_dynamic_acc 4
ok 463 - ./rma/get_acc_local 1
ok 464 - ./rma/linked_list 4
ok 465 - ./rma/linked_list_fop 4
ok 466 - ./rma/compare_and_swap 4
ok 467 - ./rma/fetch_and_op_char 4
ok 468 - ./rma/fetch_and_op_short 4
ok 469 - ./rma/fetch_and_op_int 4
ok 470 - ./rma/fetch_and_op_long 4
ok 471 - ./rma/fetch_and_op_double 4
ok 472 - ./rma/fetch_and_op_long_double 4
ok 473 - ./rma/get_accumulate_double 4
ok 474 - ./rma/get_accumulate_double_derived 4
ok 475 - ./rma/get_accumulate_int 4
ok 476 - ./rma/get_accumulate_int_derived 4
ok 477 - ./rma/get_accumulate_long 4
ok 478 - ./rma/get_accumulate_long_derived 4
ok 479 - ./rma/get_accumulate_short 4
ok 480 - ./rma/get_accumulate_short_derived 4
ok 481 - ./rma/flush 4
ok 482 - ./rma/reqops 4
ok 483 - ./rma/req_example 4
ok 484 - ./rma/req_example_shm 4
ok 485 - ./rma/rput_local_comp 2
ok 486 - ./rma/racc_local_comp 2
not ok 487 - ./rma/win_info 4
  ---
  Directory: ./rma
  File: win_info
  Num-procs: 4
  Date: "Thu Apr 13 21:16:15 2017"
  ...
## Test output (expected 'No Errors'):
## 2: no_locks is not defined
## 2: no_locks is not defined
## 1: no_locks is not defined
## 1: no_locks is not defined
## 0: no_locks is not defined
## 0: no_locks is not defined
## 3: no_locks is not defined
## 3: no_locks is not defined
ok 488 - ./rma/linked_list_lockall 4
ok 489 - ./rma/pscw_ordering 4
ok 490 - ./rma/pscw_ordering_shm 4
ok 491 - ./rma/linked_list_bench_lock_all 4
ok 492 - ./rma/linked_list_bench_lock_excl 4
ok 493 - ./rma/linked_list_bench_lock_shr 4
ok 494 - ./rma/linked_list_bench_lock_shr_nocheck 4
ok 495 - ./rma/mutex_bench 4
ok 496 - ./rma/mutex_bench_shared 4
ok 497 - ./rma/mutex_bench_shm 4
ok 498 - ./rma/mutex_bench_shm_ordered 4
ok 499 - ./rma/rma-contig 2
ok 500 - ./rma/badrma 2
ok 501 - ./rma/acc-loc 4
ok 502 - ./rma/fence_shm 2
ok 503 - ./rma/win_shared_zerobyte 4
ok 504 - ./rma/win_shared_put_flush_get 4
ok 505 - ./rma/get-struct 2
ok 506 - ./rma/at_complete 2
ok 507 - ./rma/atomic_rmw_fop 3
ok 508 - ./rma/atomic_rmw_cas 3
ok 509 - ./rma/atomic_rmw_gacc 3
ok 510 - ./rma/atomic_get 3
ok 511 - ./rma/aint 2
not ok 512 - ./rma/acc-pairtype 2
  ---
  Directory: ./rma
  File: acc-pairtype
  Num-procs: 2
  Date: "Thu Apr 13 21:16:56 2017"
  ...
## Test output (expected 'No Errors'):
ok 513 - ./rma/manyget 2
ok 514 - ./rma/derived-acc-flush_local 3
not ok 515 - ./rma/large-acc-flush_local 3
  ---
  Directory: ./rma
  File: large-acc-flush_local
  Num-procs: 3
  Date: "Thu Apr 13 21:20:00 2017"
  ...
## Test output (expected 'No Errors'):
ok 516 - ./rma/large-small-acc 2
ok 517 - ./rma/win_shared_put_flush_load 3
ok 518 - ./rma/win_shared_acc_flush_load 3
ok 519 - ./rma/win_shared_gacc_flush_load 3
ok 520 - ./rma/win_shared_fop_flush_load 3
ok 521 - ./rma/win_shared_cas_flush_load 3
ok 522 - ./rma/put_flush_get 3
ok 523 - ./rma/acc_flush_get 3
ok 524 - ./rma/gacc_flush_get 3
ok 525 - ./rma/fop_flush_get 3
ok 526 - ./rma/cas_flush_get 3
not ok 527 - ./spawn/namepub 2
  ---
  Directory: ./spawn
  File: namepub
  Num-procs: 2
  Date: "Thu Apr 13 21:20:04 2017"
  ...
## Test output (expected 'No Errors'):
## Error in Publish_name: "Invalid service name (see MPI_Publish_name), error stack:
## MPI_Publish_name(133): MPI_Publish_name(service="MyTest", MPI_INFO_NULL, port="otherhost:122") failed
## MPID_NS_Publish(59)..: Lookup failed for service name MyTest"
## [cli_0]: getval cmd failed
## Error in Lookup name: "Invalid service name (see MPI_Publish_name), error stack:
## MPI_Lookup_name(149): MPI_Lookup_name(service="MyTest", MPI_INFO_NULL, port=0x7ffd30456330) failed
## MPI_Lookup_name(129): 
## MPID_NS_Lookup(85)..: Lookup failed for service name MyTest"
## Lookup name returned name after it was unpublished
## [cli_0]: expecting cmd=lookup_result, got unpublish_result
## [cli_1]: getval cmd failed
## Lookup name returned name after it was unpublished
##  Found 4 errors
## [cli_1]: expecting cmd=finalize_ack, got lookup_result
## readFromPMIClient: lost connection to the PMI client
## [cli_0]: getval cmd failed
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## readFromPMIClient: lost connection to the PMI client
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## __PMI_send(r1): failed sending cmd=finalize_ack
##  from pmi_finalize:1357
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## __PMI_send(r0): failed sending cmd=finalize_ack
##  from pmi_finalize:1357
ok 528 - ./spawn/spawn1 1
ok 529 - ./spawn/spawn2 1
not ok 530 - ./spawn/spawninfo1 1
  ---
  Directory: ./spawn
  File: spawninfo1
  Num-procs: 1
  Date: "Thu Apr 13 21:20:05 2017"
  ...
## Test output (expected 'No Errors'):
not ok 531 - ./spawn/spawnminfo1 1
  ---
  Directory: ./spawn
  File: spawnminfo1
  Num-procs: 1
  Date: "Thu Apr 13 21:20:06 2017"
  ...
## Test output (expected 'No Errors'):
ok 532 - ./spawn/spawnintra 1
ok 533 - ./spawn/spawnintra 2
ok 534 - ./spawn/spawnargv 1
ok 535 - ./spawn/spawnmanyarg 1
ok 536 - ./spawn/spawnmult2 2
ok 537 - ./spawn/spaconacc 1
ok 538 - ./spawn/spaconacc2 1
ok 539 - ./spawn/selfconacc 2
ok 540 - ./spawn/spaiccreate 2
ok 541 - ./spawn/taskmaster 1
ok 542 - ./spawn/taskmaster 2
ok 543 - ./spawn/join 2
ok 544 - ./spawn/disconnect_reconnect 3
ok 545 - ./spawn/disconnect_reconnect2 3
ok 546 - ./spawn/disconnect_reconnect3 3
ok 547 - ./spawn/multiple_ports 3
ok 548 - ./spawn/multiple_ports2 4
ok 549 - ./spawn/disconnect 3
ok 550 - ./spawn/disconnect2 3
ok 551 - ./spawn/disconnect3 3
ok 552 - ./spawn/concurrent_spawns 1
ok 553 - ./spawn/pgroup_connect_test 4
ok 554 - ./spawn/pgroup_intercomm_test 4
ok 559 - ./spawn/spawn-rootargs 10  # SKIP xfail tests disabled
ok 555 - ./topo/cartmap1 4
ok 556 - ./topo/cartzero 4
ok 557 - ./topo/cartshift1 4
ok 558 - ./topo/cartsuball 4
ok 559 - ./topo/cartcreates 4
ok 560 - ./topo/dims1 4
ok 561 - ./topo/dims2 1
ok 562 - ./topo/dims3 1
ok 563 - ./topo/dims4 1
ok 564 - ./topo/graphmap1 4
ok 565 - ./topo/topotest 4
ok 566 - ./topo/topodup 4
ok 567 - ./topo/graphcr 4
ok 568 - ./topo/graphcr2 4
ok 569 - ./topo/distgraph1 4
ok 570 - ./topo/dgraph_unwgt 4
not ok 571 - ./topo/neighb_coll 4
  ---
  Directory: ./topo
  File: neighb_coll
  Num-procs: 4
  Date: "Thu Apr 13 21:23:27 2017"
  ...
## Test output (expected 'No Errors'):
ok 572 - ./io/rdwrord 4
ok 573 - ./io/rdwrzero 4
ok 574 - ./io/getextent 2
ok 575 - ./io/setinfo 4
ok 576 - ./io/setviewcur 4
not ok 577 - ./io/i_noncontig 2
  ---
  Directory: ./io
  File: i_noncontig
  Num-procs: 2
  Date: "Thu Apr 13 21:23:30 2017"
  ...
## Test output (expected 'No Errors'):
## Deadlock detected! Process 22144 will wait forever.
## ('wait' called without outstanding send or recv requests).
## Requests:
## Sockets:
##   sock#0xe03d40 listen:    -1 demand:   0(   0)  src:(134.94.169.164,22144,(nil),r0000001) anyrecv:     0
##   Connections:
##     con#0xe067e8 type:  loop state:    open dest:(134.94.169.164,22144,0xe067e8,r0000001) recvcnt:    0
##     con#0xe069a8 type:   shm state:    open dest:(134.94.169.164,22143,0x261cb88,r0000000) recvcnt:    0
## Fds:
## 
## Reqs:5 GenReqs: (cnt:0  used:0)
## Deadlock detected! Process 22143 will wait forever.
## ('wait' called without outstanding send or recv requests).
## Requests:
## Sockets:
##   sock#0x2619d40 listen:    -1 demand:   0(   0)  src:(134.94.169.164,22143,(nil),r0000000) anyrecv:     0
##   Connections:
##     con#0x261c7e8 type:  loop state:    open dest:(134.94.169.164,22143,0x261c7e8,r0000000) recvcnt:    0
##     con#0x261cb88 type:   shm state:    open dest:(134.94.169.164,22144,0xe069a8,r0000001) recvcnt:    0
## Fds:
## 
## Reqs:5 GenReqs: (cnt:0  used:0)
not ok 578 - ./io/async 4
  ---
  Directory: ./io
  File: async
  Num-procs: 4
  Date: "Thu Apr 13 21:26:30 2017"
  ...
## Test output (expected 'No Errors'):
not ok 579 - ./io/async_any 4
  ---
  Directory: ./io
  File: async_any
  Num-procs: 4
  Date: "Thu Apr 13 21:26:30 2017"
  ...
## Test output (expected 'No Errors'):
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
ok 580 - ./io/userioerr 1
ok 581 - ./io/resized 1
ok 582 - ./io/resized2 1
ok 583 - ./io/bigtype 1
ok 584 - ./io/hindexed_io 1
ok 585 - ./io/simple_collective 1
not ok 586 - ./io/i_bigtype 1
  ---
  Directory: ./io
  File: i_bigtype
  Num-procs: 1
  Date: "Thu Apr 13 21:27:13 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## i_bigtype: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:157: MPID_Progress_test: Assertion `mpi_errno == 0' failed.
not ok 587 - ./io/i_hindexed_io 1
  ---
  Directory: ./io
  File: i_hindexed_io
  Num-procs: 1
  Date: "Thu Apr 13 21:27:13 2017"
  ...
## Test output (expected 'No Errors'):
## INTERNAL ERROR: invalid error code efefefef (Ring Index out of range) in MPIDU_Sched_progress_state:921
## i_hindexed_io: /home/clauss/Work/psmpi/mpich2/src/mpid/psp/src/mpid_progress.c:157: MPID_Progress_test: Assertion `mpi_errno == 0' failed.
not ok 588 - ./io/i_rdwrord 4
  ---
  Directory: ./io
  File: i_rdwrord
  Num-procs: 4
  Date: "Thu Apr 13 21:30:13 2017"
  ...
## Test output (expected 'No Errors'):
not ok 589 - ./io/i_setviewcur 4
  ---
  Directory: ./io
  File: i_setviewcur
  Num-procs: 4
  Date: "Thu Apr 13 21:33:14 2017"
  ...
## Test output (expected 'No Errors'):
not ok 590 - ./io/i_aggregation1 4
  ---
  Directory: ./io
  File: i_aggregation1
  Num-procs: 4
  Date: "Thu Apr 13 21:36:14 2017"
  ...
## Test output (expected 'No Errors'):
not ok 591 - ./io/i_aggregation2 4
  ---
  Directory: ./io
  File: i_aggregation2
  Num-procs: 4
  Date: "Thu Apr 13 21:39:14 2017"
  ...
## Test output (expected 'No Errors'):
not ok 592 - ./io/i_coll_test 4
  ---
  Directory: ./io
  File: i_coll_test
  Num-procs: 4
  Date: "Thu Apr 13 21:42:14 2017"
  ...
## Test output (expected 'No Errors'):
not ok 593 - ./io/i_darray_read 4
  ---
  Directory: ./io
  File: i_darray_read
  Num-procs: 4
  Date: "Thu Apr 13 21:45:14 2017"
  ...
## Test output (expected 'No Errors'):
not ok 594 - ./io/i_hindexed 4
  ---
  Directory: ./io
  File: i_hindexed
  Num-procs: 4
  Date: "Thu Apr 13 21:48:14 2017"
  ...
## Test output (expected 'No Errors'):
not ok 595 - ./io/i_noncontig_coll 2
  ---
  Directory: ./io
  File: i_noncontig_coll
  Num-procs: 2
  Date: "Thu Apr 13 21:51:15 2017"
  ...
## Test output (expected 'No Errors'):
not ok 596 - ./io/i_noncontig_coll2 4
  ---
  Directory: ./io
  File: i_noncontig_coll2
  Num-procs: 4
  Date: "Thu Apr 13 21:54:15 2017"
  ...
## Test output (expected 'No Errors'):
not ok 597 - ./io/i_types_with_zeros 2
  ---
  Directory: ./io
  File: i_types_with_zeros
  Num-procs: 2
  Date: "Thu Apr 13 21:57:15 2017"
  ...
## Test output (expected 'No Errors'):
ok 598 - ./f77/attr/attrmpi1f 1
ok 599 - ./f77/attr/baseattrf 1
ok 600 - ./f77/attr/baseattr2f 1
ok 601 - ./f77/attr/commattrf 1
ok 602 - ./f77/attr/commattr2f 1
ok 603 - ./f77/attr/commattr3f 1
ok 604 - ./f77/attr/commattr4f 1
ok 605 - ./f77/attr/typeattrf 1
ok 606 - ./f77/attr/typeattr2f 1
ok 607 - ./f77/attr/typeattr3f 1
ok 608 - ./f77/coll/uallreducef 4
ok 609 - ./f77/coll/exscanf 5
ok 610 - ./f77/coll/alltoallwf 7
ok 611 - ./f77/coll/alltoallvf 7
ok 612 - ./f77/coll/inplacef 4
ok 613 - ./f77/coll/reducelocalf 2
ok 614 - ./f77/coll/redscatf 4
ok 615 - ./f77/coll/split_typef 4
not ok 616 - ./f77/coll/nonblockingf 4
  ---
  Directory: ./f77/coll
  File: nonblockingf
  Num-procs: 4
  Date: "Thu Apr 13 22:00:20 2017"
  ...
## Test output (expected 'No Errors'):
ok 617 - ./f77/coll/vw_inplacef 4
ok 618 - ./f77/coll/red_scat_blockf 4
not ok 619 - ./f77/coll/nonblocking_inpf 4
  ---
  Directory: ./f77/coll
  File: nonblocking_inpf
  Num-procs: 4
  Date: "Thu Apr 13 22:03:20 2017"
  ...
## Test output (expected 'No Errors'):
ok 620 - ./f77/datatype/typenamef 1
ok 621 - ./f77/datatype/typename3f 1
ok 622 - ./f77/datatype/typesnamef 1
ok 623 - ./f77/datatype/typecntsf 1
ok 624 - ./f77/datatype/typem2f 1
ok 625 - ./f77/datatype/typesubf 1
ok 626 - ./f77/datatype/packef 1
ok 627 - ./f77/datatype/gaddressf 1
ok 628 - ./f77/datatype/allctypesf 1
ok 629 - ./f77/datatype/hindex1f 1
ok 630 - ./f77/datatype/hindexed_blockf 1
ok 631 - ./f77/datatype/bottom 2
ok 632 - ./f77/pt2pt/statusesf 1
not ok 633 - ./f77/pt2pt/greqf 1
  ---
  Directory: ./f77/pt2pt
  File: greqf
  Num-procs: 1
  Date: "Thu Apr 13 22:03:24 2017"
  ...
## Test output (expected 'No Errors'):
## Deadlock detected! Process 27960 will wait forever.
## ('wait' called without outstanding send or recv requests).
## Requests:
## Sockets:
##   sock#0xf4fdb0 listen:    -1 demand:   0(   0)  src:(134.94.169.164,27960,(nil),r0000000) anyrecv:     0
##   Connections:
##     con#0xf52778 type:  loop state:    open dest:(134.94.169.164,27960,0xf52778,r0000000) recvcnt:    0
## Fds:
## 
## Reqs:3 GenReqs: (cnt:0  used:0)
ok 634 - ./f77/pt2pt/allpairf 2
ok 635 - ./f77/pt2pt/mprobef 2
ok 636 - ./f77/info/infotestf 1
ok 637 - ./f77/info/infotest2f 1
not ok 638 - ./f77/spawn/namepubf 2
  ---
  Directory: ./f77/spawn
  File: namepubf
  Num-procs: 2
  Date: "Thu Apr 13 22:03:25 2017"
  ...
## Test output (expected 'No Errors'):
##  Error in Publish_name Invalid service name (see MPI_Publish_name), error stack:
## MPI_Publish_name(133): MPI_Publish_name(service="MyTest", MPI_INFO_NULL, port="otherhost:122") failed
## MPID_NS_Publish(59)..: Lookup failed for service name MyTest
##  Error in Lookup nameInvalid service name (see MPI_Publish_name), error stack:
## MPI_Lookup_name(149): MPI_Lookup_name(service="MyTest", MPI_INFO_NULL, port=0x20a1318) failed
## MPI_Lookup_name(129): 
## MPID_NS_Lookup(85)..: Lookup failed for service name MyTest
## [cli_0]: getval cmd failed
## [cli_0]: expecting cmd=lookup_result, got unpublish_result
## [cli_1]: getval cmd failed
##  Lookup name returned name after it was unpublished
##  Lookup name returned name after it was unpublished
##   Found            4  errors
## [cli_0]: getval cmd failed
## [cli_1]: expecting cmd=finalize_ack, got lookup_result
## readFromPMIClient: lost connection to the PMI client
## readFromPMIClient: lost connection to the PMI client
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## __PMI_send(r0): failed sending cmd=finalize_ack
##  from pmi_finalize:1357
## __PMI_send(r1): failed sending cmd=finalize_ack
##  from pmi_finalize:1357
ok 639 - ./f77/spawn/spawnf 1
ok 640 - ./f77/spawn/spawnargvf 1
not ok 641 - ./f77/spawn/connaccf 2
  ---
  Directory: ./f77/spawn
  File: connaccf
  Num-procs: 2
  Date: "Thu Apr 13 22:03:25 2017"
  ...
## Test output (expected 'No Errors'):
## [cli_1]: getval cmd failed
##  lookup name returned a value before published
## [cli_0]: getval cmd failed
## Fatal error in MPI_Lookup_name: Invalid service name (see MPI_Publish_name), error stack:
## MPI_Lookup_name(149): MPI_Lookup_name(service="fservtest", MPI_INFO_NULL, port=0xfd8ba8) failed
## MPI_Lookup_name(129): 
## MPID_NS_Lookup(85)..: Lookup failed for service name fservtest
not ok 642 - ./f77/spawn/spawnmultf 1
  ---
  Directory: ./f77/spawn
  File: spawnmultf
  Num-procs: 1
  Date: "Thu Apr 13 22:03:26 2017"
  ...
## Test output (expected 'No Errors'):
##  Found arg a                                                                                but expected -p        
##  Found arg b=c                                                                              but expected 27        
##  Found arg d e                                                                              but expected -echo     
##  Wrong number of arguments (           5 )
##   Found            4  errors
ok 643 - ./f77/spawn/spawnmult2f 2
not ok 644 - ./f77/io/iwriteatf 4
  ---
  Directory: ./f77/io
  File: iwriteatf
  Num-procs: 4
  Date: "Thu Apr 13 22:03:26 2017"
  ...
## Test output (expected 'No Errors'):
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## #0  0x7FAEF1B8ED47
## #0  0x7FAA55C17D47
## #0  0x7FD1848B0D47
## #0  0x7F659F1E7D47
## #1  0x7FAEF1B8F34E
## #1  0x7FAA55C1834E
## #1  0x7FD1848B134E
## #1  0x7F659F1E834E
## #2  0x7FAEF10AA11F
## #2  0x7FD183DCC11F
## #2  0x7F659E70311F
## #2  0x7FAA5513311F
## #3  0x7F659E74F2FA
## #4  0x7F659E73791E
## #3  0x7FAA5517F2FA
## #3  0x7FAEF10F62FA
## #3  0x7FD183E182FA
## #4  0x7FAEF10DE91E
## #4  0x7FD183E0091E
## #4  0x7FAA5516791E
## #5  0x7FAA5602C924
## #6  0x7FAA56023C92
## #5  0x7F659F5FC924
## #5  0x7FAEF1FA3924
## #6  0x7F659F5F3C92
## #6  0x7FAEF1F9AC92
## #7  0x7F659F5EF433
## #8  0x7F659F5AE279
## #7  0x7FAEF1F96433
## #9  0x7F659F5AE7BA
## #8  0x7FAEF1F55279
## #10  0x7F659F96153D
## #9  0x7FAEF1F557BA
## #7  0x7FAA5601F433
## #8  0x7FAA55FDE279
## #9  0x7FAA55FDE7BA
## #10  0x7FAA5639153D
## #11  0x4017FA in MAIN__ at iwriteatf.f:109
## #10  0x7FAEF230853D
## #11  0x4017FA in MAIN__ at iwriteatf.f:109
## #11  0x4017FA in MAIN__ at iwriteatf.f:109
not ok 645 - ./f77/io/iwritef 4
  ---
  Directory: ./f77/io
  File: iwritef
  Num-procs: 4
  Date: "Thu Apr 13 22:03:26 2017"
  ...
## Test output (expected 'No Errors'):
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## #0  0x7FD48C29DD47
## #0  0x7F73036EFD47
## #0  0x7FEC70CC6D47
## #0  0x7FC513C4ED47
## #1  0x7FD48C29E34E
## #1  0x7F73036F034E
## #1  0x7FEC70CC734E
## #1  0x7FC513C4F34E
## #2  0x7FD48B7B911F
## #2  0x7F7302C0B11F
## #2  0x7FC51316A11F
## #3  0x7FD48B8052FA
## #3  0x7F7302C572FA
## #4  0x7F7302C3F91E
## #3  0x7FC5131B62FA
## #4  0x7FD48B7ED91E
## #4  0x7FC51319E91E
## #5  0x7FD48C6B2924
## #5  0x7FC514063924
## #6  0x7FC51405AC92
## #6  0x7FD48C6A9C92
## #2  0x7FEC701E211F
## #3  0x7FEC7022E2FA
## #4  0x7FEC7021691E
## #5  0x7FEC710DB924
## #6  0x7FEC710D2C92
## #7  0x7FC514056433
## #7  0x7FD48C6A5433
## #5  0x7F7303B04924
## #6  0x7F7303AFBC92
## #7  0x7F7303AF7433
## #8  0x7FC514015279
## #8  0x7FD48C664279
## #9  0x7FC5140157BA
## #8  0x7F7303AB6279
## #9  0x7F7303AB67BA
## #7  0x7FEC710CE433
## #8  0x7FEC7108D279
## #9  0x7FEC7108D7BA
## #9  0x7FD48C6647BA
## #10  0x7FC5143C853D
## #10  0x7FD48CA1753D
## #10  0x7FEC7144053D
## #10  0x7F7303E6953D
## #11  0x4018CE in MAIN__ at iwritef.f:124
## #11  0x4018CE in MAIN__ at iwritef.f:124
## #11  0x4018CE in MAIN__ at iwritef.f:124
## #11  0x4018CE in MAIN__ at iwritef.f:124
not ok 646 - ./f77/io/iwriteshf 4
  ---
  Directory: ./f77/io
  File: iwriteshf
  Num-procs: 4
  Date: "Thu Apr 13 22:06:27 2017"
  ...
## Test output (expected 'No Errors'):
ok 647 - ./f77/io/writef 4
ok 648 - ./f77/io/writeatf 4
ok 649 - ./f77/io/writeallf 4
ok 650 - ./f77/io/writeshf 4
ok 651 - ./f77/io/writeordf 4
ok 652 - ./f77/io/writeatallf 4
ok 653 - ./f77/io/writeatallbef 4
ok 654 - ./f77/io/writeallbef 4
ok 655 - ./f77/io/writeordbef 4
ok 656 - ./f77/io/fileerrf 1
ok 657 - ./f77/io/fileinfof 3
ok 658 - ./f77/io/shpositionf 4
ok 659 - ./f77/io/atomicityf 8
ok 660 - ./f77/io/miscfilef 4
ok 661 - ./f77/io/setviewcurf 4
ok 662 - ./f77/io/c2f2ciof 1
ok 663 - ./f77/io/c2fmultio 1
not ok 664 - ./f77/io/i_setviewcurf 4
  ---
  Directory: ./f77/io
  File: i_setviewcurf
  Num-procs: 4
  Date: "Thu Apr 13 22:09:34 2017"
  ...
## Test output (expected 'No Errors'):
not ok 665 - ./f77/io/iwriteatallf 4
  ---
  Directory: ./f77/io
  File: iwriteatallf
  Num-procs: 4
  Date: "Thu Apr 13 22:12:34 2017"
  ...
## Test output (expected 'No Errors'):
ok 666 - ./f77/rma/winscale1f 4
ok 667 - ./f77/rma/winfencef 4
ok 668 - ./f77/rma/wingetf 5
ok 669 - ./f77/rma/winscale2f 4
ok 670 - ./f77/rma/winerrf 1
ok 671 - ./f77/rma/winnamef 1
ok 672 - ./f77/rma/wingroupf 4
ok 673 - ./f77/rma/winaccf 4
ok 674 - ./f77/rma/c2f2cwinf 1
ok 675 - ./f77/rma/baseattrwinf 1
ok 676 - ./f77/rma/winattrf 1
ok 677 - ./f77/rma/winattr2f 1
ok 678 - ./f77/rma/aintf 2
ok 679 - ./f77/init/baseenvf 1
ok 680 - ./f77/comm/commnamef 2
ok 681 - ./f77/comm/commerrf 2
ok 682 - ./f77/ext/c2f2cf 1
ok 683 - ./f77/ext/c2fmult 1
ok 684 - ./f77/ext/ctypesinf 1
ok 685 - ./f77/ext/allocmemf 1
ok 686 - ./f77/topo/cartcrf 4
ok 687 - ./f77/topo/dgraph_wgtf 4
ok 688 - ./f77/topo/dgraph_unwgtf 4
ok 689 - ./f77/profile/profile1f 2
ok 690 - ./cxx/attr/attrtx 2
ok 691 - ./cxx/attr/attricx 4
ok 692 - ./cxx/attr/baseattrcommx 1
ok 693 - ./cxx/attr/fkeyvalcommx 1
ok 694 - ./cxx/attr/fkeyvaltypex 1
ok 695 - ./cxx/pt2pt/bsend1cxx 2
ok 696 - ./cxx/pt2pt/sendrecvx 2
ok 697 - ./cxx/comm/commname2 4
ok 698 - ./cxx/coll/arcomplex 4
ok 699 - ./cxx/coll/uallredx 5
ok 700 - ./cxx/coll/uallreduce 5
ok 701 - ./cxx/coll/ureduce 5
ok 702 - ./cxx/coll/ureducelocal 5
ok 703 - ./cxx/coll/uscan 5
ok 704 - ./cxx/coll/uexscan 5
ok 705 - ./cxx/coll/alltoallw2x 10
ok 706 - ./cxx/coll/icbcastx 4
ok 707 - ./cxx/coll/icbcastx 10
ok 708 - ./cxx/coll/icallreducex 5
ok 709 - ./cxx/coll/icreducex 5
ok 710 - ./cxx/coll/icscatterx 5
ok 711 - ./cxx/coll/icgatherx 5
ok 712 - ./cxx/coll/icallgatherx 5
ok 713 - ./cxx/coll/icbarrierx 5
ok 714 - ./cxx/coll/icallgathervx 5
ok 715 - ./cxx/coll/icgathervx 5
ok 716 - ./cxx/coll/icscattervx 5
ok 717 - ./cxx/coll/icalltoallx 5
ok 718 - ./cxx/coll/reduceboolx 5
ok 719 - ./cxx/coll/redscatblk 4
ok 720 - ./cxx/errhan/commcallx 2
ok 721 - ./cxx/init/baseenv 1
ok 722 - ./cxx/init/initstatx 1
ok 723 - ./cxx/init/initstat2x 1
ok 724 - ./cxx/info/infodupx 1
ok 725 - ./cxx/info/infodelx 1
ok 726 - ./cxx/info/infovallenx 1
ok 727 - ./cxx/info/infoorderx 1
ok 728 - ./cxx/datatype/packsizex 1
ok 729 - ./cxx/datatype/typecntsx 1
ok 730 - ./cxx/datatype/typenamex 1
ok 731 - ./cxx/datatype/typemiscx 1
not ok 732 - ./cxx/io/iwriteatx 4
  ---
  Directory: ./cxx/io
  File: iwriteatx
  Num-procs: 4
  Date: "Thu Apr 13 22:12:57 2017"
  ...
## Test output (expected 'No Errors'):
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
not ok 733 - ./cxx/io/iwritex 4
  ---
  Directory: ./cxx/io
  File: iwritex
  Num-procs: 4
  Date: "Thu Apr 13 22:12:58 2017"
  ...
## Test output (expected 'No Errors'):
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
not ok 734 - ./cxx/io/iwriteshx 4
  ---
  Directory: ./cxx/io
  File: iwriteshx
  Num-procs: 4
  Date: "Thu Apr 13 22:15:58 2017"
  ...
## Test output (expected 'No Errors'):
ok 735 - ./cxx/io/writex 4
ok 736 - ./cxx/io/writeatx 4
ok 737 - ./cxx/io/writeallx 4
ok 738 - ./cxx/io/writeshx 4
ok 739 - ./cxx/io/writeordx 4
ok 740 - ./cxx/io/writeatallx 4
ok 741 - ./cxx/io/writeatallbex 4
ok 742 - ./cxx/io/writeallbex 4
ok 743 - ./cxx/io/writeordbex 4
not ok 744 - ./cxx/io/iwriteatnosx 4
  ---
  Directory: ./cxx/io
  File: iwriteatnosx
  Num-procs: 4
  Date: "Thu Apr 13 22:16:06 2017"
  ...
## Test output (expected 'No Errors'):
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
not ok 745 - ./cxx/io/iwritenosx 4
  ---
  Directory: ./cxx/io
  File: iwritenosx
  Num-procs: 4
  Date: "Thu Apr 13 22:16:06 2017"
  ...
## Test output (expected 'No Errors'):
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
not ok 746 - ./cxx/io/iwriteshnosx 4
  ---
  Directory: ./cxx/io
  File: iwriteshnosx
  Num-procs: 4
  Date: "Thu Apr 13 22:19:07 2017"
  ...
## Test output (expected 'No Errors'):
ok 747 - ./cxx/io/writenosx 4
ok 748 - ./cxx/io/writeatnosx 4
ok 749 - ./cxx/io/writeallnosx 4
ok 750 - ./cxx/io/writeshnosx 4
ok 751 - ./cxx/io/writeordnosx 4
ok 752 - ./cxx/io/writeatallnosx 4
ok 753 - ./cxx/io/writeatallbenosx 4
ok 754 - ./cxx/io/writeallbenosx 4
ok 755 - ./cxx/io/writeordbenosx 4
ok 756 - ./cxx/io/fileerrx 1
ok 757 - ./cxx/io/fileinfox 3
ok 758 - ./cxx/io/filemiscx 4
ok 759 - ./cxx/io/shpositionx 4
ok 760 - ./cxx/io/seekavail 1
not ok 761 - ./cxx/spawn/namepubx 2
  ---
  Directory: ./cxx/spawn
  File: namepubx
  Num-procs: 2
  Date: "Thu Apr 13 22:19:17 2017"
  ...
## Test output (expected 'No Errors'):
## Error in Publish_name Invalid service name (see MPI_Publish_name), error stack:
## MPI_Publish_name(133): MPI_Publish_name(service="MyTest", MPI_INFO_NULL, port="otherhost:122") failed
## MPID_NS_Publish(59)..: Lookup failed for service name MyTest
## Error in Lookup name Invalid service name (see MPI_Publish_name), error stack:
## MPI_Lookup_name(149): MPI_Lookup_name(service="MyTest", MPI_INFO_NULL, port=0x1ec5840) failed
## MPI_Lookup_name(129): 
## MPID_NS_Lookup(85)..: Lookup failed for service name MyTest
## [cli_1]: getval cmd failed
## [cli_0]: getval cmd failed
## Lookup name returned name after it was unpublished
## The name returned was 
## [cli_0]: expecting cmd=lookup_result, got unpublish_result
## Lookup name returned name after it was unpublished
## The name returned was 
##  Found 4 errors
## [cli_0]: getval cmd failed
## readFromPMIClient: lost connection to the PMI client
## [cli_1]: expecting cmd=finalize_ack, got lookup_result
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## readFromPMIClient: lost connection to the PMI client
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## __PMI_send(r0): failed sending cmd=finalize_ack
##  from pmi_finalize:1357
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## __PMI_send(r1): failed sending cmd=finalize_ack
##  from pmi_finalize:1357
ok 762 - ./cxx/spawn/spawnintrax 1
ok 763 - ./cxx/spawn/spawnintrax 2
ok 764 - ./cxx/spawn/spawnargvx 1
ok 765 - ./cxx/spawn/selfconaccx 2
ok 766 - ./cxx/rma/winnamex 1
ok 767 - ./cxx/rma/wincallx 1
ok 768 - ./cxx/rma/getgroupx 4
ok 769 - ./cxx/rma/winfencex 4
ok 770 - ./cxx/rma/winscale1x 4
ok 771 - ./cxx/rma/winscale2x 4
ok 772 - ./cxx/rma/fkeyvalwinx 1
ok 773 - ./f90/attr/attrmpi1f90 1
ok 774 - ./f90/attr/baseattrf90 1
ok 775 - ./f90/attr/baseattr2f90 1
ok 776 - ./f90/attr/commattrf90 1
ok 777 - ./f90/attr/commattr2f90 1
ok 778 - ./f90/attr/commattr3f90 1
ok 779 - ./f90/attr/commattr4f90 1
ok 780 - ./f90/attr/typeattrf90 1
ok 781 - ./f90/attr/typeattr2f90 1
ok 782 - ./f90/attr/typeattr3f90 1
ok 783 - ./f90/attr/fandcattrf90 1
ok 784 - ./f90/attr/baseattr3f90 1
ok 785 - ./f90/attr/attrlangf90 1
ok 786 - ./f90/coll/uallreducef90 4
ok 787 - ./f90/coll/exscanf90 5
ok 788 - ./f90/coll/alltoallwf90 7
ok 789 - ./f90/coll/alltoallvf90 7
ok 790 - ./f90/coll/inplacef90 4
ok 791 - ./f90/coll/reducelocalf90 2
ok 792 - ./f90/coll/redscatf90 4
ok 793 - ./f90/coll/split_typef90 4
not ok 794 - ./f90/coll/nonblockingf90 4
  ---
  Directory: ./f90/coll
  File: nonblockingf90
  Num-procs: 4
  Date: "Thu Apr 13 22:22:26 2017"
  ...
## Test output (expected 'No Errors'):
ok 795 - ./f90/coll/vw_inplacef90 4
ok 796 - ./f90/coll/red_scat_blockf90 4
not ok 797 - ./f90/coll/nonblocking_inpf90 4
  ---
  Directory: ./f90/coll
  File: nonblocking_inpf90
  Num-procs: 4
  Date: "Thu Apr 13 22:25:26 2017"
  ...
## Test output (expected 'No Errors'):
ok 798 - ./f90/comm/commnamef90 2
ok 799 - ./f90/comm/commerrf90 2
ok 800 - ./f90/ext/c2f2cf90 1
ok 801 - ./f90/ext/c2f90mult 1
ok 802 - ./f90/ext/ctypesinf90 1
ok 803 - ./f90/ext/allocmemf90 1
ok 804 - ./f90/info/infotestf90 1
ok 805 - ./f90/info/infotest2f90 1
ok 806 - ./f90/init/baseenvf90 1
not ok 807 - ./f90/io/iwriteatf90 4
  ---
  Directory: ./f90/io
  File: iwriteatf90
  Num-procs: 4
  Date: "Thu Apr 13 22:25:29 2017"
  ...
## Test output (expected 'No Errors'):
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## 
## Backtrace for this error:
## #0  0x7F7557312D47
## #0  0x7FF5F68C5D47
## #0  0x7FFA3E786D47
## #0  0x7FB9347E2D47
## #1  0x7F755731334E
## #1  0x7FF5F68C634E
## #1  0x7FFA3E78734E
## #1  0x7FB9347E334E
## #2  0x7F755682E11F
## #2  0x7FFA3DCA211F
## #2  0x7FB933CFE11F
## #3  0x7F755687A2FA
## #3  0x7FFA3DCEE2FA
## #4  0x7F755686291E
## #4  0x7FFA3DCD691E
## #3  0x7FB933D4A2FA
## #4  0x7FB933D3291E
## #5  0x7F7557727924
## #5  0x7FFA3EB9B924
## #6  0x7FFA3EB92C92
## #6  0x7F755771EC92
## #2  0x7FF5F5DE111F
## #3  0x7FF5F5E2D2FA
## #4  0x7FF5F5E1591E
## #5  0x7FF5F6CDA924
## #6  0x7FF5F6CD1C92
## #7  0x7FFA3EB8E433
## #7  0x7F755771A433
## #5  0x7FB934BF7924
## #6  0x7FB934BEEC92
## #7  0x7FB934BEA433
## #8  0x7FFA3EB4D279
## #8  0x7F75576D9279
## #7  0x7FF5F6CCD433
## #8  0x7FF5F6C8C279
## #9  0x7FF5F6C8C7BA
## #9  0x7F75576D97BA
## #9  0x7FFA3EB4D7BA
## #10  0x7FFA3EF0053D
## #8  0x7FB934BA9279
## #9  0x7FB934BA97BA
## #10  0x7F7557A8C53D
## #10  0x7FF5F703F53D
## #10  0x7FB934F5C53D
## #11  0x4017D3 in MAIN__ at iwriteatf90.f90:107
## #11  0x4017D3 in MAIN__ at iwriteatf90.f90:107
## #11  0x4017D3 in MAIN__ at iwriteatf90.f90:107
## #11  0x4017D3 in MAIN__ at iwriteatf90.f90:107
not ok 808 - ./f90/io/iwritef90 4
  ---
  Directory: ./f90/io
  File: iwritef90
  Num-procs: 4
  Date: "Thu Apr 13 22:25:29 2017"
  ...
## Test output (expected 'No Errors'):
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## Assertion failed in file /home/clauss/Work/psmpi/mpich2/src/mpi/pt2pt/mpir_request.c at line 680: MPID_Request_is_complete(request_ptrs[i])
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## 
## Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
## 
## Backtrace for this error:
## #0  0x7F74D8E98D47
## #0  0x7F64FA0E5D47
## #0  0x7F41295C4D47
## #0  0x7FA3B4600D47
## #1  0x7F74D8E9934E
## #1  0x7F64FA0E634E
## #1  0x7F41295C534E
## #2  0x7F74D83B411F
## #2  0x7F64F960111F
## #3  0x7F74D84002FA
## #3  0x7F64F964D2FA
## #4  0x7F74D83E891E
## #1  0x7FA3B460134E
## #2  0x7FA3B3B1C11F
## #3  0x7FA3B3B682FA
## #4  0x7FA3B3B5091E
## #5  0x7FA3B4A15924
## #5  0x7F74D92AD924
## #6  0x7FA3B4A0CC92
## #6  0x7F74D92A4C92
## #7  0x7FA3B4A08433
## #8  0x7FA3B49C7279
## #7  0x7F74D92A0433
## #8  0x7F74D925F279
## #9  0x7FA3B49C77BA
## #9  0x7F74D925F7BA
## #10  0x7F74D961253D
## #10  0x7FA3B4D7A53D
## #2  0x7F4128AE011F
## #3  0x7F4128B2C2FA
## #4  0x7F4128B1491E
## #5  0x7F41299D9924
## #6  0x7F41299D0C92
## #7  0x7F41299CC433
## #8  0x7F412998B279
## #9  0x7F412998B7BA
## #10  0x7F4129D3E53D
## #11  0x4018A7 in MAIN__ at iwritef90.f90:122
## #11  0x4018A7 in MAIN__ at iwritef90.f90:122
## #11  0x4018A7 in MAIN__ at iwritef90.f90:122
## #4  0x7F64F963591E
## #5  0x7F64FA4FA924
## #6  0x7F64FA4F1C92
## #7  0x7F64FA4ED433
## #8  0x7F64FA4AC279
## #9  0x7F64FA4AC7BA
## #10  0x7F64FA85F53D
## #11  0x4018A7 in MAIN__ at iwritef90.f90:122
not ok 809 - ./f90/io/iwriteshf90 4
  ---
  Directory: ./f90/io
  File: iwriteshf90
  Num-procs: 4
  Date: "Thu Apr 13 22:28:29 2017"
  ...
## Test output (expected 'No Errors'):
ok 810 - ./f90/io/writef90 4
ok 811 - ./f90/io/writeatf90 4
ok 812 - ./f90/io/writeallf90 4
ok 813 - ./f90/io/writeshf90 4
ok 814 - ./f90/io/writeordf90 4
ok 815 - ./f90/io/writeatallf90 4
ok 816 - ./f90/io/writeatallbef90 4
ok 817 - ./f90/io/writeallbef90 4
ok 818 - ./f90/io/writeordbef90 4
ok 819 - ./f90/io/fileerrf90 1
ok 820 - ./f90/io/fileinfof90 3
ok 821 - ./f90/io/shpositionf90 4
ok 822 - ./f90/io/atomicityf90 8
ok 823 - ./f90/io/miscfilef90 4
ok 824 - ./f90/io/setviewcurf90 4
ok 825 - ./f90/io/c2f2ciof90 1
ok 826 - ./f90/io/c2f90multio 1
not ok 827 - ./f90/io/i_setviewcurf90 4
  ---
  Directory: ./f90/io
  File: i_setviewcurf90
  Num-procs: 4
  Date: "Thu Apr 13 22:31:36 2017"
  ...
## Test output (expected 'No Errors'):
not ok 828 - ./f90/io/iwriteatallf90 4
  ---
  Directory: ./f90/io
  File: iwriteatallf90
  Num-procs: 4
  Date: "Thu Apr 13 22:34:36 2017"
  ...
## Test output (expected 'No Errors'):
ok 829 - ./f90/misc/sizeof2 1
ok 830 - ./f90/pt2pt/statusesf90 1
not ok 831 - ./f90/pt2pt/greqf90 1
  ---
  Directory: ./f90/pt2pt
  File: greqf90
  Num-procs: 1
  Date: "Thu Apr 13 22:34:38 2017"
  ...
## Test output (expected 'No Errors'):
## Deadlock detected! Process 8311 will wait forever.
## ('wait' called without outstanding send or recv requests).
## Requests:
## Sockets:
##   sock#0x2285db0 listen:    -1 demand:   0(   0)  src:(134.94.169.164,8311,(nil),r0000000) anyrecv:     0
##   Connections:
##     con#0x2288778 type:  loop state:    open dest:(134.94.169.164,8311,0x2288778,r0000000) recvcnt:    0
## Fds:
## 
## Reqs:3 GenReqs: (cnt:0  used:0)
ok 832 - ./f90/pt2pt/allpairf90 2
ok 833 - ./f90/pt2pt/mprobef90 2
ok 834 - ./f90/datatype/typenamef90 1
ok 835 - ./f90/datatype/typename3f90 1
ok 836 - ./f90/datatype/typesnamef90 1
ok 837 - ./f90/datatype/typecntsf90 1
ok 838 - ./f90/datatype/typem2f90 1
ok 839 - ./f90/datatype/typesubf90 1
ok 840 - ./f90/datatype/packef90 1
ok 841 - ./f90/datatype/gaddressf90 1
ok 842 - ./f90/datatype/allctypesf90 1
ok 843 - ./f90/datatype/hindex1f90 1
ok 844 - ./f90/datatype/hindexed_blockf90 1
ok 845 - ./f90/datatype/bottom 2
ok 846 - ./f90/datatype/structf 2
ok 847 - ./f90/datatype/indtype 2
ok 848 - ./f90/datatype/createf90 1
ok 849 - ./f90/datatype/sizeof 1
ok 850 - ./f90/datatype/kinds 2
ok 851 - ./f90/datatype/trf90 1
ok 852 - ./f90/datatype/get_elem_d 2
ok 853 - ./f90/datatype/get_elem_u 2
ok 854 - ./f90/f90types/createf90types 1
ok 855 - ./f90/f90types/createf90types 1
ok 856 - ./f90/rma/winscale1f90 4
ok 857 - ./f90/rma/winfencef90 4
ok 858 - ./f90/rma/wingetf90 5
ok 859 - ./f90/rma/winscale2f90 4
ok 860 - ./f90/rma/winerrf90 1
ok 861 - ./f90/rma/winnamef90 1
ok 862 - ./f90/rma/wingroupf90 4
ok 863 - ./f90/rma/winaccf90 4
ok 864 - ./f90/rma/c2f2cwinf90 1
ok 865 - ./f90/rma/baseattrwinf90 1
ok 866 - ./f90/rma/winattrf90 1
ok 867 - ./f90/rma/winattr2f90 1
ok 868 - ./f90/rma/aintf90 2
not ok 869 - ./f90/spawn/namepubf90 2
  ---
  Directory: ./f90/spawn
  File: namepubf90
  Num-procs: 2
  Date: "Thu Apr 13 22:34:46 2017"
  ...
## Test output (expected 'No Errors'):
##  Error in Publish_name Invalid service name (see MPI_Publish_name), error stack:
## MPI_Publish_name(133): MPI_Publish_name(service="MyTest", MPI_INFO_NULL, port="otherhost:122") failed
## MPID_NS_Publish(59)..: Lookup failed for service name MyTest
##  Error in Lookup nameInvalid service name (see MPI_Publish_name), error stack:
## MPI_Lookup_name(149): MPI_Lookup_name(service="MyTest", MPI_INFO_NULL, port=0x18db318) failed
## MPI_Lookup_name(129): 
## MPID_NS_Lookup(85)..: Lookup failed for service name MyTest
## [cli_0]: getval cmd failed
## [cli_1]: getval cmd failed
## [cli_0]: expecting cmd=lookup_result, got unpublish_result
##  Lookup name returned name after it was unpublished
##  Lookup name returned name after it was unpublished
##   Found            4  errors
## readFromPMIClient: lost connection to the PMI client
## [cli_0]: getval cmd failed
## readFromPMIClient: lost connection to the PMI client
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## [cli_1]: expecting cmd=finalize_ack, got lookup_result
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## __PMI_send(r0): failed sending cmd=finalize_ack
##  from pmi_finalize:1357
## do_send(r1): got error 9 on PMI socket: Bad file descriptor
## __PMI_send(r1): failed sending cmd=finalize_ack
##  from pmi_finalize:1357
ok 870 - ./f90/spawn/spawnf90 1
ok 871 - ./f90/spawn/spawnargvf90 1
not ok 872 - ./f90/spawn/connaccf90 2
  ---
  Directory: ./f90/spawn
  File: connaccf90
  Num-procs: 2
  Date: "Thu Apr 13 22:34:46 2017"
  ...
## Test output (expected 'No Errors'):
## [cli_1]: getval cmd failed
## [cli_0]: getval cmd failed
##  lookup name returned a value before published
## Fatal error in MPI_Lookup_name: Invalid service name (see MPI_Publish_name), error stack:
## MPI_Lookup_name(149): MPI_Lookup_name(service="fservtest", MPI_INFO_NULL, port=0x1715ba8) failed
## MPI_Lookup_name(129): 
## MPID_NS_Lookup(85)..: Lookup failed for service name fservtest
not ok 873 - ./f90/spawn/spawnmultf90 1
  ---
  Directory: ./f90/spawn
  File: spawnmultf90
  Num-procs: 1
  Date: "Thu Apr 13 22:34:46 2017"
  ...
## Test output (expected 'No Errors'):
##  Found arg a                                                                                but expected -p        
##  Found arg b=c                                                                              but expected 27        
##  Found arg d e                                                                              but expected -echo     
##  Wrong number of arguments (           5 )
##   Found            4  errors
ok 874 - ./f90/spawn/spawnmult2f90 2
ok 875 - ./f90/spawn/spawnargvf03 1
not ok 876 - ./f90/spawn/spawnmultf03 1
  ---
  Directory: ./f90/spawn
  File: spawnmultf03
  Num-procs: 1
  Date: "Thu Apr 13 22:34:47 2017"
  ...
## Test output (expected 'No Errors'):
##  Found arg a                                                                                but expected -p        
##  Found arg b=c                                                                              but expected 27        
##  Found arg d e                                                                              but expected -echo     
##  Wrong number of arguments (           5 )
##   Found            4  errors
ok 877 - ./f90/timer/wtimef90 1
ok 878 - ./f90/topo/cartcrf90 4
ok 879 - ./f90/topo/dgraph_wgtf90 4
ok 880 - ./f90/topo/dgraph_unwgtf90 4
ok 881 - ./f90/profile/profile1f90 2
ok 882 - ./errors/attr/keyvalmis 1
ok 883 - ./errors/coll/noalias 2
ok 889 - ./errors/coll/nb_noalias 2  # SKIP xfail tests disabled
ok 884 - ./errors/coll/noalias2 4
ok 885 - ./errors/coll/noalias3 4
ok 886 - ./errors/coll/rerr 2
ok 887 - ./errors/coll/nb_rerr 2
ok 888 - ./errors/coll/reduce_local 1
not ok 889 - ./errors/coll/bcastlength 4
  ---
  Directory: ./errors/coll
  File: bcastlength
  Num-procs: 4
  Date: "Thu Apr 13 22:37:50 2017"
  ...
## Test output (expected 'No Errors'):
ok 896 - ./errors/coll/ibcastlength 4  # SKIP xfail tests disabled
ok 890 - ./errors/comm/cfree 4
ok 891 - ./errors/comm/ccreate1 8
ok 892 - ./errors/comm/userdup 4
ok 893 - ./errors/comm/manysplit 4
ok 894 - ./errors/comm/too_many_comms 4
not ok 895 - ./errors/comm/too_many_icomms 4
  ---
  Directory: ./errors/comm
  File: too_many_icomms
  Num-procs: 4
  Date: "Thu Apr 13 22:40:51 2017"
  ...
## Test output (expected 'No Errors'):
ok 896 - ./errors/comm/too_many_comms2 4
ok 897 - ./errors/comm/too_many_comms3 4
not ok 898 - ./errors/comm/too_many_icomms2 4
  ---
  Directory: ./errors/comm
  File: too_many_icomms2
  Num-procs: 4
  Date: "Thu Apr 13 22:43:54 2017"
  ...
## Test output (expected 'No Errors'):
ok 899 - ./errors/group/gerr 1
ok 900 - ./errors/pt2pt/proberank 1
ok 901 - ./errors/pt2pt/truncmsg1 2
ok 902 - ./errors/pt2pt/truncmsg2 2
ok 903 - ./errors/pt2pt/errinstatts 2
ok 904 - ./errors/pt2pt/errinstatta 2
ok 905 - ./errors/pt2pt/errinstatws 2
ok 906 - ./errors/pt2pt/errinstatwa 2
ok 907 - ./errors/topo/cartsmall 4
ok 908 - ./errors/rma/winerr 2
ok 909 - ./errors/rma/winerr2 2
ok 910 - ./errors/rma/cas_type_check 2
ok 911 - ./errors/rma/win_sync_unlock 2
ok 912 - ./errors/rma/win_sync_free_pt 2
ok 913 - ./errors/rma/win_sync_free_at 2
ok 914 - ./errors/rma/win_sync_complete 2
ok 915 - ./errors/rma/win_sync_lock_at 2
ok 916 - ./errors/rma/win_sync_lock_pt 2
ok 917 - ./errors/rma/win_sync_lock_fence 2
ok 918 - ./errors/rma/win_sync_nested 2
ok 919 - ./errors/rma/win_sync_op 2
ok 920 - ./errors/spawn/badport 2
not ok 921 - ./errors/spawn/unpub 1
  ---
  Directory: ./errors/spawn
  File: unpub
  Num-procs: 1
  Date: "Thu Apr 13 22:44:00 2017"
  ...
## Test output (expected 'No Errors'):
##  No Errors
## [cli_0]: getval cmd failed
## readFromPMIClient: lost connection to the PMI client
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## __PMI_send(r0): failed sending cmd=finalize_ack
##  from pmi_finalize:1357
not ok 922 - ./errors/spawn/lookup_name 1
  ---
  Directory: ./errors/spawn
  File: lookup_name
  Num-procs: 1
  Date: "Thu Apr 13 22:44:00 2017"
  ...
## Test output (expected 'No Errors'):
## [cli_0]: getval cmd failed
## [cli_0]: getval cmd failed
## [cli_0]: getval cmd failed
## [cli_0]: getval cmd failed
## [cli_0]: getval cmd failed
## [cli_0]: getval cmd failed
## [cli_0]: getval cmd failed
## [cli_0]: getval cmd failed
## [cli_0]: getval cmd failed
## [cli_0]: getval cmd failed
##  No Errors
## [cli_0]: expecting cmd=finalize_ack, got lookup_result
## readFromPMIClient: lost connection to the PMI client
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## do_send(r0): got error 9 on PMI socket: Bad file descriptor
## __PMI_send(r0): failed sending cmd=finalize_ack
##  from pmi_finalize:1357
ok 923 - ./errors/io/fileerrret 1
ok 924 - ./errors/io/openerr 1
ok 925 - ./errors/io/file_errhdl 1
ok 926 - ./errors/f77/io/uerrhandf 1
ok 927 - ./errors/cxx/errhan/errgetx 1
ok 928 - ./errors/cxx/errhan/errsetx 1
ok 929 - ./errors/cxx/errhan/throwtest 1
ok 930 - ./errors/cxx/errhan/commerrx 2
ok 931 - ./errors/cxx/io/fileerrretx 1
ok 932 - ./errors/cxx/io/errgetfilex 1
ok 933 - ./errors/cxx/io/throwtestfilex 1
ok 934 - ./errors/f90/io/uerrhandf90 1
1..941
